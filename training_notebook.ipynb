{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/karthikrangasai/efficient_face.git@master\n",
    "! pip install torchtext==0.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "import pytorch_lightning.callbacks as plcb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from efficient_face.data import ciFAIRDataModule\n",
    "from efficient_face.models import SoftmaxBasedModel, TripletLossBasedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, Adadelta, Adagrad, RMSprop\n",
    "from torch_optimizer import Ranger, Lookahead, SGDW\n",
    "from torch.optim.lr_scheduler import (\n",
    "    ConstantLR,\n",
    "    CosineAnnealingWarmRestarts,\n",
    "    CyclicLR,\n",
    "    StepLR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1234\n",
    "\n",
    "# Optimizer Params\n",
    "LEARNING_RATE = 1e-3\n",
    "OPTIMIZER_CLS = Adam\n",
    "OPTIMIZER_KWARGS = dict()  # Don't add `params` and `lr` arguments here\n",
    "LR_SCHEDULER_CLS = None\n",
    "LR_SCHEDULER_KWARGS = dict(\n",
    "    num_steps_arg=None,  # Change this value to the argument name when changing LR Scheduler\n",
    "    num_steps_factor=1.0,\n",
    ")\n",
    "\n",
    "# DataModule Params\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Model Params\n",
    "MODEL_NAME = \"mobilenetv3_small_100\"\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "# Loss Function Params\n",
    "DISTANCE_METRIC = \"L2\"\n",
    "TRIPLET_STRATEGY = \"VANILLA\"\n",
    "MINER_KWARGS = dict()\n",
    "LOSS_FUNC_KWARGS = dict()\n",
    "\n",
    "# Trainer Params\n",
    "ACCELERATOR = \"gpu\"  # or \"cpu\"\n",
    "NUM_DEVICES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup SEED for Random generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = ciFAIRDataModule(\n",
    "    batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxBasedModel(\n",
    "    model_name=MODEL_NAME,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    distance_metric=DISTANCE_METRIC,\n",
    "    triplet_strategy=TRIPLET_STRATEGY,\n",
    "    miner_kwargs=MINER_KWARGS,\n",
    "    loss_func_kwargs=LOSS_FUNC_KWARGS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    optimizer=OPTIMIZER_CLS,\n",
    "    optimizer_kwargs=OPTIMIZER_KWARGS,\n",
    "    lr_scheduler=LR_SCHEDULER_CLS,\n",
    "    lr_scheduler_kwargs=LR_SCHEDULER_KWARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = plcb.RichModelSummary()\n",
    "progress_bar = plcb.RichProgressBar()\n",
    "lr_monitor = plcb.LearningRateMonitor(logging_interval=\"step\")\n",
    "checkpoint = plcb.ModelCheckpoint(\n",
    "    dirpath=\"\",\n",
    "    filename=\"{epoch}--{val_loss:.3f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_last=True,\n",
    "    save_top_k=2,\n",
    "    mode=\"min\",\n",
    "    auto_insert_metric_name=True,\n",
    "    every_n_epochs=2,\n",
    ")\n",
    "\n",
    "CALLBACKS = [model_summary, progress_bar, lr_monitor, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = WandbLogger(\n",
    "    project=\"efficient_face\",\n",
    "    log_model=True,\n",
    "    group=MODEL_NAME,\n",
    "    id=None,  # Change when a run has failed to auto-resume it.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    num_sanity_val_steps=0,\n",
    "    check_val_every_n_epoch=2,\n",
    "    detect_anomaly=True,\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    accelerator=ACCELERATOR,\n",
    "    devices=NUM_DEVICES,\n",
    "    logger=LOGGER,\n",
    "    callbacks=CALLBACKS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint.best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('bhanu_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "44ed58201659f32b3cbb1fb3b9bd3ec77148fd8605c765955dcee957e4eb946e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
